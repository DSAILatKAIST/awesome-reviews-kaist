---
description : Y Bai et al., / Are Transformers More Robust Than CNNs? / Neurips-2021
---

# **Are Transformers More Robust Than CNNs?** 


## **1. Problem Definition**  

- Vision Transformer(ViT) NetworkëŠ” CNNë³´ë‹¤ ê°•ë ¥í•˜ê³  robustí•˜ë‹¤ê³  ì•Œë ¤ì ¸ìˆë‹¤.
- í•˜ì§€ë§Œ ì´ ì—°êµ¬ì—ì„œëŠ” ëª‡ê°€ì§€ ì‹¤í—˜ì„ í†µí•´ì„œ ê¸°ì¡´ì˜ ì´ëŸ° ë¯¿ìŒì— ì˜ë¬¸ì„ ì œê¸°í•˜ê³  ê³µì •í•˜ê²Œ ì„¤ê³„ëœ ì‹¤í—˜ì¡°ê±´ì—ì„œ ê°•ê±´ì„±ì„ ë‹¤ì‹œ ì¡°ì‚¬í•œë‹¤.
- ê²°ë¡ ì ìœ¼ë¡œ adversarial attackì— CNNë„ ì¶©ë¶„íˆ ê°•ê±´í•  ìˆ˜ ìˆìŒì„ í™•ì¸í–ˆë‹¤
- ê°•ê±´ì„±ì—ëŒ€í•œ ì‹¤í—˜ ë„ì¤‘ì—, ë°©ëŒ€í•œ ì–‘ì˜ ë°ì´í„°ë¥¼ ì‚¬ìš©í•œ pre-trainingì´ transformerê°€ CNNì˜ ì„±ëŠ¥ì„ ë„˜ëŠ”ë° ê¼­ í•„ìš”í•œ ê²ƒì€ ì•„ë‹˜ë„ ë¶€ê°€ì ìœ¼ë¡œ í™•ì¸í–ˆë‹¤.


## **2. Motivation**  

- Pure-attention based modelì¸ transformerê°€ inductive biasì—†ì´ CNNì˜ ì„±ëŠ¥ì„ ë›°ì–´ë„˜ì—ˆê³  Detection, instance segmentation, sementic segmentationì—ì„œë„ ì—°êµ¬ë˜ê³ ìˆë‹¤
- ë˜í•œ ìµœê·¼ ì—°êµ¬ë“¤ì—ì„œ TransformerëŠ” OODì™€ ì ëŒ€ì  ê³µê²©ì— CNNë³´ë‹¤ ê°•ê±´í•¨ì´ ë°í˜€ì¡Œë‹¤
    - *í•˜ì§€ë§Œ*, ì €ìëŠ” ì´ëŸ° ê²°ê³¼ê°€ unfairí•œ í™˜ê²½ì—ì„œ ë„ì¶œë˜ì—ˆë‹¤ê³  ì£¼ì¥í•œë‹¤
    - #paramsê°€ Transformerìª½ì´ ë§ì•˜ê³  training dataset, epochs and augmentation ì „ëµ ë“±ì´ ë™ì¼í•˜ê²Œ ë§ì¶°ì§€ì§€ ì•Šì•˜ë‹¤(ë’¤ì— ì‹¤í—˜ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆë“¯ì´ ViTì—ê²Œ ìœ ë¦¬í•œ ì¡°ê±´ì´ ë‹¤ìˆ˜ ìˆë‹¤)
- ì´ ì—°êµ¬ì—ì„œ ê³µì •í•œ ë¹„êµë¥¼ í†µí•´ ì ëŒ€ì  ê³µê²©ê³¼ OODì— ëŒ€í•œ ê°•ê±´ì„±ì„ í™•ì¸í•  ê²ƒì´ë‹¤
    - CNNì´ Transformerì˜ training recipesë¥¼ ë”°ë¥¸ë‹¤ë©´ perturbationê³¼ patchì— ê¸°ë°˜í•œ attackì— ë” ê°•ê±´í•¨ì„ ë°œê²¬í–ˆë‹¤
    - ì—¬ì „íˆ Transformerê°€ OODì— ê°•ê±´í•¨ì„ ë°œê²¬í–ˆê³  ì´ëŠ” pre-trainingì´ ì—†ì–´ë„ ê°€ëŠ¥í–ˆë‹¤. Ablation studyì—ì„œ self-attentionì´ ì´ëŸ° í˜„ìƒì˜ ì´ìœ ì„ì„ ë°œê²¬í–ˆë‹¤

<aside>

ğŸ’¡  ì´ ì—°êµ¬ê°€ ë‹¤ë¥¸ Architectureë¼ë¦¬ì˜ ê°•ê±´ì„±ì„ ë¹„êµí•˜ëŠ” í‘œì¤€ì´ ë˜ê¸¸ ë°”ë€ë‹¤ê³  ì €ìëŠ” ë°íˆê³  ìˆìŠµë‹ˆë‹¤

</aside>



## **3. Method**  
- ì´ ì±•í„°ì—ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë‚´ìš©ì„ ë‹¤ë£¬ë‹¤. ëª¨ë‘ ì‹¤í—˜ì—ì„œ ìì£¼ ë“±ì¥í•  ë‚´ìš©ì´ë¯€ë¡œ ì£¼ì˜ê¹Šê²Œ ìˆ™ì§€í•˜ê¸¸ ë°”ëë‹ˆë‹¤.
1. CNNê³¼ ViTì˜ í•™ìŠµì¡°ê±´ ë¹„êµ
2. ë‹¤ì–‘í•œ Attackê³¼ OOD Dataset

## 3.1 Training CNNs and Transformer

- í•™ìŠµ í›„ CNNì™€ ViTì˜ Top-1 AccëŠ” 76.8, 76.9ë¡œ ë§¤ìš° ë¹„ìŠ·í•œ ì„±ëŠ¥ì„ ëƒ„

### CNN

- ResNet-50ì´ ViTì™€ ë¹„ìŠ·í•œ #paramsë¥¼ ê°€ì§€ë¯€ë¡œ ì±„íƒ
- ImageNetì— í•™ìŠµ
- ê¸°íƒ€ í•™ìŠµ ë””í…Œì¼(SGD-momentum, 100eph, L2ê·œì œ)

### ViT

- ì™¸ë¶€ ë°ì´í„°ì—†ì´ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚¸ DeiTì˜ recipeë¥¼ ë”°ë¼ì„œ DeiT-S(#paramsê°€ ResNet50ê³¼ ë¹„ìŠ·)ë¥¼ default ViTë¡œ ì±„íƒí•¨
- AdamW, 3ê°œì˜ Aug(Rand, Cut, MixUp)
- ResNetê³¼ í•™ìŠµ í™˜ê²½ì„ ë§ì¶”ê¸°ìœ„í•´ Erasing, Stochastic Depth, Repeated Augë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ. DeiTëŠ” 300ephí•™ìŠµë˜ì§€ë§Œ ê°™ì€ ì´ìœ ë¡œ 100ephë§Œ í•™ìŠµ

## 3.2 Robustness Evaluations

### 3.2.1 Adversarial Attack
#### PGD
- PGD(Projected Gradient Descent) : ì‚¬ëŒì€ í™•ì¸í•˜ê¸° ì–´ë µì§€ë§Œ ê¸°ê³„ë¥¼ ì†ì¼ ìˆ˜ ìˆëŠ” ì„­ë™

![Untitled](https://erratic-tailor-f01.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F42d2c8ef-1b52-4718-a081-9f6d2426de53%2FUntitled.png?table=block&id=c8d0616a-d5f1-492b-8c77-a31b94d5b362&spaceId=ad2a71b5-1b0d-4734-bbc4-60a807442e5d&width=2000&userId=&cache=v2)

#### TPA
- TPA : textureê°€ ìˆëŠ” patchë¥¼ ë¶™ì—¬ ë„¤íŠ¸ì›Œí¬ë¥¼ ì†ì´ëŠ” attack


![Untitled](https://erratic-tailor-f01.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fc37dcc0d-a43c-4f71-a9c6-cf7f25bf73e8%2FUntitled.png?table=block&id=0da98098-a42c-45d4-98ee-f9187cb9e2cd&spaceId=ad2a71b5-1b0d-4734-bbc4-60a807442e5d&width=1030&userId=&cache=v2)
        
![Untitled](https://erratic-tailor-f01.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F6764d1d3-4b81-4d2b-9bb1-87e945c2d3c4%2FUntitled.png?table=block&id=2687d828-3a4f-4982-804a-c8119aa82f0f&spaceId=ad2a71b5-1b0d-4734-bbc4-60a807442e5d&width=1610&userId=&cache=v2)
        
### 3.2.2 OOD 
- ë…¼ë¬¸ê³¼ PaperWithCodeì— ìˆëŠ” ì„¤ëª…ì´ ì¡°ê¸ˆ ë‹¤ë¥¸ë° PWCë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì ì—ˆë‹¤
    - ImageNet-A : ResNet modelì´ ê°•í•œ í™•ì‹ ìœ¼ë¡œ í‹€ë¦° ì´ë¯¸ì§€ì…‹. ê¸°ê³„í•™ìŠµ ëª¨ë¸ì´ ì–´ë ¤ì›Œí•˜ëŠ” ì¦‰ í•™ìŠµ ë¶„í¬ë‘ì€ ì¢€ ë‹¤ë¥¸ ì´ë¯¸ì§€ë“¤ì˜ ëª¨ì„ì´ë‹¤. ì‹¤ì œ ì´ë¯¸ì§€ë¥¼ ë³´ë©´ ì™œ ê·¸ëŸ° í‹€ë¦° ë‹µì„ ëƒˆëŠ”ì§€ ì•Œ ê²ƒë„ ê°™ë‹¤
    - ImageNet-A ì˜ˆì‹œ
        
        ![Untitled](%5BPresentation%5DAre%20Transformers%20More%20Robust%20Than%20CN%20c8d0616ad5f1492b8c77a31b94d5b362/Untitled%203.png)
        
    - ImageNet-C : ì´ë¯¸ì§€ë‹¹ ë‹¤ì–‘í•œ Augmentationì´ ì ìš©ëœ ì´ë¯¸ì§€ì…‹
    - ImageNet-C ì˜ˆì‹œ
        
        ![Untitled](%5BPresentation%5DAre%20Transformers%20More%20Robust%20Than%20CN%20c8d0616ad5f1492b8c77a31b94d5b362/Untitled%204.png)
        
    - Stylized ImageNet :  ì´ë¯¸ì§€ë‹¹ ë‹¤ì–‘í•œ textureë¥¼ ì…í•œ ë°ì´í„°ì…‹
    - Stylized ImageNet ì˜ˆì‹œ
        
        ![Untitled](%5BPresentation%5DAre%20Transformers%20More%20Robust%20Than%20CN%20c8d0616ad5f1492b8c77a31b94d5b362/Untitled%205.png)
        






## **4. Experiment**  

In this section, please write the overall experiment results.  
At first, write experiment setup that should be composed of contents.  

### **Experiment setup**  
* Dataset  
* baseline  
* Evaluation Metric  

### **Result**  
Then, show the experiment results which demonstrate the proposed method.  
You can attach the tables or figures, but you don't have to cover all the results.  
  



## **5. Conclusion**  

- unfairí•œ ì¡°ê±´ì—ì„œ ì‹¤í–‰ë˜ë˜ ì‹¤í—˜ì„ ì ì ˆí•œ ì¡°ì¹˜ë¥¼ í†µí•´ ë¹„êµí•˜ë‹ˆ TransformerëŠ” ì ëŒ€ì  ê³µê²©ì—ì„œ CNNë³´ë‹¤ ê°•ê±´í•˜ì§€ ì•Šì•˜ë‹¤
- ë˜í•œ OODì—ì„œì˜ Transformerì„±ëŠ¥ì€ self-attentionê³¼ ê´€ë ¨ì´ ìˆìŒì„ í™•ì¸í–ˆë‹¤
- ì´ ì—°êµ¬ë¡œ transformerì— ëŒ€í•œ ì´í•´ê°€ í–¥ìƒë˜ê³  transformerê³¼ CNNì‚¬ì´ ê³µì •í•œ ë¹„êµê°€ ê°€ëŠ¥í•´ì§€ê¸¸ ë°”ë€ë‹¤

### ê°œì¸ì  ì˜ê²¬ìœ¼ë¡œ..
- ViTì˜ ë“±ì¥ì€ ë§ì€ ì´ìŠˆë¥¼ ë‚³ì•˜ìŠµë‹ˆë‹¤. ì²˜ìŒ CNNì´í›„ Imageë¶„ë¥˜ë¥¼ ìœ„í•œ ê·¼ì›ì ì¸ ìƒˆë¡œìš´ ë°©ë²•ë¡  ì œì‹œì˜€ê³  ë¬´ì—‡ë³´ë‹¤ ì„±ëŠ¥ì´ ì¢‹ì•˜ìŠµë‹ˆë‹¤. ì‹¬ì§€ì–´ ìµœê·¼ ì—°êµ¬ë“¤ì—ì„œëŠ” ViTê°€ CNNë³´ë‹¤ ê°•ê±´í•˜ê¸°ê¹Œì§€ í•˜ë‹¤ëŠ” ê²°ê³¼ë¥¼ ë„ì¶œí•˜ë©´ì„œ Visionì˜ ì˜ì—­ì€ ì´ì œ (ì—„ì²­ë‚œ pretrain datasetì„ ê°€ì§„ ì‚¬ì—…ì²´ê°€ í•™ìŠµí•œ) ViTê°€ ëª¨ë‘ ê°€ì ¸ê°ˆ ê²ƒì´ë¼ëŠ” ì˜ˆìƒì„ í•˜ê¸°ë„ í–ˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ í•™ê³„ì˜ ì´ëŸ° ë¯¿ìŒ ìì²´ì— ì˜ë¬¸ì„ ê°€ì§€ê³  ë„ì „í•˜ëŠ”ê²Œ ì‰¬ìš´ì¼ì´ ì•„ë‹ˆì—ˆì„ ê²ƒì´ë¼ê³  ìƒê°í•©ë‹ˆë‹¤. ì´ëŸ° ì—°êµ¬ë¥¼ ë‚´ë†“ì€ ì—°êµ¬ìë“¤ì˜ ì‹¤ë ¥ê³¼ ìì‹ ê°ì—ì„œ ë˜ í•œë²ˆ ê²¸ì†í•´ì•¼í•¨ì„ ëŠë‚ë‹ˆë‹¤.


---  
## **Author Information**  

* í™ì„±ë˜ SungRae Hong
    * Master's Course, KAIST Knowledge Service Engineering 
    * Interested In : SSL, Vision DL, Audio DL
    * Contact : sun.hong@kaist.ac.kr

## **6. Reference & Additional materials**  

Please write the reference. If paper provides the public code or other materials, refer them.  

* Github Implementation  
* Reference  

