---
description : Hanxiao Liu, Zihang Dai, David R. So, Quoc V. Le / Pay Attention to MLPs / NeurIPS - 2021
---

# Pay Attention to MLPs


## 1. Problem Definition

- gatingê³¼ MLP êµ¬ì¡°ë¥¼ ì‚¬ìš©í•œ ê°„ë‹¨í•œ ë„¤íŠ¸ì›Œí¬ë¥¼ í†µí•´ Transformerì™€ ë¹„ìŠ·í•œ ì„±ëŠ¥ì„ ë³´ì´ëŠ” gMLP ì œì•ˆ

## 2. Motivation
-   _**Motivation**_
    -   TransformerëŠ” ë‘ ê°€ì§€ ì¤‘ìš”í•œ ë¶€ë¶„ì„ ê°€ì§€ê³  ìˆìŒ
        -   ê° í† í°ì´ ë³‘ë ¬ë¡œ ì²˜ë¦¬ ë  ìˆ˜ ìˆëŠ” recurrent-free í•œ ì•„í‚¤í…ì³
        -   multi-head self attentionì„ í†µí•´, í† í°ë“¤ ê°„ì˜ ì •ë³´ë¥¼ í†µí•©
    -   ì—¬ê¸°ì„œ attention êµ¬ì¡°ëŠ” ì¸í’‹ ê°„ì˜ ê´€ê³„ì— ë”°ë¼ íŒŒë¼ë¯¸í„°ê°€ ë‹¬ë¼ì§€ë©´ì„œ í† í°ë“¤ì˜ ê´€ê³„ë¥¼ íŒë‹¨í•˜ëŠ” inductive bias ë¥¼ ê°€ì§€ê³  ìˆìŒ
    -   ê·¸ëŸ°ë°, ê³¼ì—° self-attention êµ¬ì¡°ê°€ Transformerì˜ ì„±ëŠ¥ì— ì¤‘ìš”í•œ ì—­í• ì„ í–ˆì„ê¹Œ?
        -   self-attentionì´ ì •ë§ í•„ìš”í• ì§€ì— ëŒ€í•œ ì˜ë¬¸ì—ì„œ ì—°êµ¬ê°€ ì‹œì‘
 - _**Idea**_ 
    -   self-attention êµ¬ì¡°ê°€ ì—†ëŠ” MLP ê¸°ë°˜ ëª¨ë¸ ì œì•ˆ (basic MLP layers with gating)

## 3. Method
>ğŸ“Œ **ê´€ë ¨ ìš©ì–´**
>-  **Identity Mapping**  
>    - ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°„ ê°’ xê°€ ì–´ë– í•œ í•¨ìˆ˜ë¥¼ í†µê³¼í•˜ë”ë¼ë„ ë‹¤ì‹œ xê°€ ë‚˜ì™€ì•¼ í•œë‹¤ëŠ” ê²ƒ

  ### 3.1 Model description
  - ë™ì¼í•œ í¬ê¸°ì™€ êµ¬ì¡°ì¸ Lê°œì˜ blocksìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆìŒ    
-   ê° ë¸”ë¡ì€ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ë¨
- ![](https://latex.codecogs.com/svg.image?Z&space;=&space;\sigma&space;(XU),&space;\widetilde{Z}&space;=&space;s(Z),&space;Y&space;=&space;\widetilde{Z}V)
- ![](https://latex.codecogs.com/svg.image?X&space;\in&space;\mathbb{R}^{n\times&space;d})ì€  ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ nì´ê³  ì°¨ì›ì´ dì¸ token representations
-  ![](https://latex.codecogs.com/svg.image?\sigma&space;)ì€ activation function (such as GeLU)
-  Uì™€ VëŠ” channel dimensionìƒì—ì„œì˜ linear projection
- s( )ëŠ” spatial interactionì„ ìºì¹˜í•  ìˆ˜ ìˆëŠ” ë ˆì´ì–´
- sê°€ indentity mappingì¼ ë•Œ, së¥¼ í†µê³¼í•˜ë©´ regular Feed forward neural networkë¡œ ë³€í•¨
  - ì—¬ê¸°ì„œ ê° tokenì´ ì–´ë–¤ ìƒí˜¸ì‘ìš© ì—†ì´ ë…ë¦½ì ìœ¼ë¡œ ì²˜ë¦¬ë¨ (<->  transformer ì™€ ë‹¬ë¦¬)
 - ë³¸ ì—°êµ¬ì—ì„œ ì£¼ìš” focus ì¤‘ í•˜ë‚˜ëŠ” ë°”ë¡œ ì¶©ë¶„íˆ ë³µì¡í•œ spatial interactionì„ í¬ì°©í•  ìˆ˜ ìˆëŠ” ì¢‹ì€ së¥¼ ë””ìì¸ í•˜ëŠ” ê²ƒ! 
 - ì „ë°˜ì ì¸ ë¸”ë¡ ë ˆì´ì•„ì›ƒì€ inverted bottlenecksì—ì„œ ì˜ê°ì„ ì–»ì–´ spatial depthwise convolutionìœ¼ë¡œ êµ¬ìƒ
 - **Transformerì™€ ë‹¬ë¦¬, ë³¸ ì—°êµ¬ì—ì„œ ì œì‹œí•œ ëª¨ë¸ì€ position embeddingì„ ìš”êµ¬í•˜ì§€ ì•ŠìŒ**
    - ì™œëƒí•˜ë©´, ì´ëŸ° ì •ë³´ê°€ s( )ì—ì„œ í¬ì°©ë  ê²ƒì´ê¸° ë•Œë¬¸
 - ë³¸ ì—°êµ¬ì—ì„œì˜ ëª¨ë¸ì€ BERT / ViTì™€ ë™ì¼í•œ input, output formatì„ ì‚¬ìš©í•¨
    - ì˜ˆì‹œ) language taskë¥¼ í•  ë•Œ, multiple text segmentsë¥¼ concatí•˜ê³  predictionì´ ë§ˆì§€ë§‰ ë ˆì´ì–´ representationì„ í†µí•´ ë„ì¶œë¨ (ì´ëŸ¬í•œ êµ¬ì¡°ê°€ ë™ì¼í•˜ê²Œ ë³¸ ì—°êµ¬ì—ì„œì˜ ëª¨ë¸ì—ë„ ì“°ì¸ë‹¤ëŠ” ì˜ë¯¸)

### 3.2 Spatial Gating Unit
- ![](https://latex.codecogs.com/svg.image?s(Z)&space;=&space;Z&space;\odot&space;f_{W,b}(Z))
  - elementwise multiplication
   - ì´ë•Œ, f(Z)ëŠ” ë‹¨ìˆœí•œ linear projectionìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ìŒ 
     -  ![](https://latex.codecogs.com/svg.image?f_{W,b}(Z)&space;=&space;WZ&space;&plus;&space;b)
     - WëŠ” n * n í–‰ë ¬ (n: ì‹œí€€ìŠ¤ ê¸¸ì´)
- í•™ìŠµì˜ ì•ˆì •ì„±ì„ ìœ„í•´, Wë¥¼ 0ìœ¼ë¡œ, bë¥¼ 1ë¡œ initialize í•´ì£¼ëŠ” ê²ƒ ì¤‘ìš”í•¨
  - ![](https://latex.codecogs.com/svg.image?f_{W,b}(Z)&space;\approx&space;1) , ![](https://latex.codecogs.com/svg.image?s(Z)&space;\approx&space;Z) 
- ì´ëŸ° ì´ˆê¸°í™”ê°€ ëª¨ë¸ì˜ ê° ë¸”ëŸ­ì´ í•™ìŠµ ì´ˆê¸° ë‹¨ê³„ì—ì„œ regular FFNì²˜ëŸ¼ í–‰ë™í•˜ë„ë¡ í•¨
- ë˜, s(Z)ë¥¼ ì—°ì‚°í•  ë•Œ, Zë¥¼ ë‚˜ëˆ„ì–´ ì—°ì‚°í•˜ëŠ” ê²ƒì´ ë” íš¨ìœ¨ì ì´ë¼ê³  í•¨
	- ![](https://latex.codecogs.com/svg.image?s(Z)&space;=&space;Z_1&space;\odot&space;f_{W,b}(Z_2))
- gMLP Overview ---> ê·¸ë¦¼ ë„£ê¸°

## 4. Experiment
- ë³¸ ë…¼ë¬¸ì—ì„œëŠ” í¬ê²Œ 2ê°€ì§€ ë¶„ì•¼ì—ì„œ gMLP ê²€ì¦
	- Image classification
	- Language Modeling

### 4.1 Image classification
- gMLP ëª¨ë¸ê³¼ attentive model ë“¤ ì‚¬ì´ ë¹„êµ
	- Vision Transformer (ViT)
	- DeiT (ViT with improved regularization)
	- several other representative convolutional networks
- Architecture specifications of gMLP models for vision -> ì‚¬ì§„ ë„£ê¸°
- **Results**
	![](https://blog.kakaocdn.net/dn/bicySA/btq6k7MqyPc/ZMSXD6336qnrTgtUfPsoy1/img.png)
	- ìœ„ ê²°ê³¼ë¥¼ í†µí•´ gMLPsê°€ DeiTì™€ ê²¬ì¤„ë§Œ í•˜ë‹¤ëŠ” ê²ƒì„ ë³´ì„
		- ì¦‰, self-attentionì´ ì—†ëŠ” ëª¨ë¸ë„ Transformerë§Œí¼ efficient í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŒ
		
### 4.2   Masked Language Modeling with BERT
- masked language modeling (MLM) task ì‹¤í—˜ ì§„í–‰
- input/output í˜•ì‹ì€ BERTë¥¼ ë”°ë¦„
- Transformerì™€ ë‹¤ë¥¸ ì ì€ positional embeddingì„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ê²ƒ
- ablation & case studyì—ì„œ ëª¨ë¸ì€ batch size 2048, max length 128 for 125K stepsë¡œ í•™ìŠµë˜ì—ˆìŒ
- main experimentsëŠ” batch size 256, max length 512 ë¡œ í•™ìŠµë¨

#### 4.2.1 Ablation:  The Importance of Gating in gMLP for BERTâ€™s Pretraining
- gMLP ì—¬ëŸ¬ê°œì˜ ë²„ì „ê³¼ baselines ë¹„êµ
- ì‚¬ìš©í•œ baselines
	- ê¸°ì¡´ BERT 
	- ê¸°ì¡´ BERT  +  relative position biases
	- (ê¸°ì¡´ BERT  +  relative position biases) - softmaxì—ì„œ ëª¨ë“  content-dependent terms ì œê±° 
		-  variant of Transformers without self-attentionë¼ê³  ë³¼ ìˆ˜ ìˆìŒ
	- Tranformerì—ì„œì˜ multi-head attentionì„ ëŒ€ì²´í•˜ëŠ” MLP-Mixer ëª¨ë¸
	- Metricìœ¼ë¡œ ì–¸ì–´ëª¨ë¸ì˜ ì„±ëŠ¥ì„ íŒë‹¨í•  ìˆ˜ ìˆëŠ” ì§€í‘œì¸ perplexityë¥¼ ì‚¬ìš©
		- ê°’ì´ ë‚®ì„ ìˆ˜ë¡ ëª¨ë¸ì´ ì˜ í•™ìŠµë˜ì—ˆë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸
- **Results**![](https://blog.kakaocdn.net/dn/Kv8U6/btq6kGIDnAE/fmMkEESonK1UBXsAeWFuMK/img.png)
	- gMLP with SGUê°€ Transformer ë§Œí¼ì˜ perplexityë¥¼ ì–»ìŒ

#### 4.2.2  Case Study: The Behavior of gMLP as Model Size Increases
- ëª¨ë¸ì˜ í¬ê¸°ê°€ ì»¤ì§ˆ ë•Œë§ˆë‹¤ ì„±ëŠ¥ì´ ì–´ë–»ê²Œ ë³€í•˜ëŠ”ì§€ í™•ì¸í•˜ì˜€ìŒ
- **Results**![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FxbTHF%2Fbtq6hUHp0Yx%2FAGS2oh5zNLovKErlex6Y9K%2Fimg.png)
  - gMLP ëª¨ë¸ì´ deep í•´ì§ˆ ìˆ˜ë¡ Transformerì™€ ì„±ëŠ¥ì´ ë¹„ìŠ·í•´ì§€ë©°, ì‹¬ì§€ì–´ëŠ” outperform í•˜ëŠ” ê²½ìš°ë„ ì¡´ì¬í•¨
 - ì´ë¯¸ì§€ ì¶”ê°€
	 -  SST-2, ì¦‰ sentiment analysis task ì¸¡ë©´ì—ì„œë„ ê°™ì€ ê°œìˆ˜ì˜ íŒŒë¼ë¯¸í„°ì¼ ë•Œ, gMLP ëª¨ë¸ì´ Transformer ë³´ë‹¤ ë›°ì–´ë‚˜ë‹¤ëŠ” ê²ƒì„ ë³´ì„
	 - í•˜ì§€ë§Œ, natural language inference task ì¸¡ë©´ì—ì„œëŠ” Transformerê°€ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì˜€ìŒ
		 - ì™œëƒí•˜ë©´, ì´ taskì—ì„œëŠ” ëª¨ë¸ì´ 2 ë¬¸ì¥ì„ ë‹¤ë£¨ì–´ì•¼ í•˜ëŠ”ë°, ì—¬ê¸°ì„œ self-attention ì´ ìœ ìš©í•œ ì—­í• ì„ í–ˆê¸° ë•Œë¬¸ -> self-attentionì„ ê°€ì§€ê³  ìˆëŠ” Transformer ê°€ í›¨ì”¬ ìœ ë¦¬



#### 4.2.3 Ablation: The Usefulness of Tiny Attention in BERTâ€™s Finetuning

- ìœ„ì˜ MNLI-m ê²°ê³¼ì—ì„œ gMLPê°€ Transformer ëª¨ë¸ë³´ë‹¤ ì„±ëŠ¥ì´ ë‚®ì•˜ë˜ ê²ƒì„ ê°œì„ í•˜ê¸° ìœ„í•´ tiny self-attention blockì„ ëª¨ë¸ì— ì¶”ê°€í•˜ì˜€ìŒ
	- ì´ë¯¸ gMLPê°€ spatial ê´€ê³„ë¥¼ íŒŒì•…í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— self-attention ëª¨ë“ˆì´ heavy í•  í•„ìš”ëŠ” ì—†ë‹¤ê³  ìƒê°
	- ì‚¬ì´ì¦ˆê°€ 64ì¸ single head -> "aMLP" ë¼ê³  í•¨
- ì‚¬ì§„ ì¶”ê°€ ...
	- aMLPê°€ ëª¨ë‘ Transformer ë³´ë‹¤ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì„



#### 4.2.4 Main Results for MLM in the BERT Setup

- full BERT setupì—ì„œ ì‹¤í—˜ì„ ì§„í–‰í•¨
- ê³µì •í•œ ë¹„êµë¥¼ ìœ„í•´ gMLPì˜ depthì™€ widthë¥¼ ì¡°ì •í•´ì¤Œ
- ì´ë¯¸ì§€ ì¶”ê°€ (ëª¨ë¸ specification)
- ì‹¤í—˜ ê²°ê³¼ ì´ë¯¸ì§€ ì¶”ê°€
	- gMLPê°€ Transformerì™€ ê²¬ì¤„ë§Œ í•˜ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŒ
	- ì•ì„œ ê³„ì† ì–¸ê¸‰í–ˆì§€ë§Œ, gMLPì˜ í¬ê¸°ê°€ ì»¤ì§ˆ ìˆ˜ë¡ Transformerì™€ì˜ ì„±ëŠ¥ ê°­ì´ ì¤„ì–´ë“œëŠ” ê²ƒì„ í™•ì¸ í•  ìˆ˜ ìˆìŒ
	- ë˜, tiny single-head self-attentionì„ ì‚¬ìš©í•˜ê¸°ë§Œ í•´ë„ Transformerë³´ë‹¤ ì¢‹ì€ ì„±ëŠ¥ì´ ë‚˜ì˜¬ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŒ



## Conclusion

- ë³¸ ì—°êµ¬ëŠ” Transformerì˜ self-attentionì´ ì¤‘ìš”í•œ ì—­í• ì„ í•˜ëŠ”ê°€ì—ì„œ ì‹œì‘í•˜ì—¬ ì—°êµ¬ë¥¼ ì§„í–‰í•¨
- ë”°ë¼ì„œ ë³¸ ì—°êµ¬ì—ì„œëŠ” multi-head self-attention layerë¥¼ ëŒ€ì‹ í•  ìˆ˜ ìˆëŠ” ê°„ë‹¨í•œ ë°©ë²•ì„ ì œì‹œ
	- gMLPs, a simple variant of MLPs with gating
- gMLPëŠ” íŠ¹ì • ë¶„ì•¼ì—ì„œ Transformerë³´ë‹¤ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ ìˆìŒì„ ë³´ì„
- ë˜, Transformerì˜ multi-head self-attentionì´ ë¬¸ì¥ê³¼ì˜ ê´€ê³„ë¥¼ ê³ ë ¤í•˜ëŠ” taskì—ì„œ ìœ ìš©í•¨ì„ ì•Œ ìˆ˜ ìˆì—ˆìŒ
- ëª¨ë¸ ì‚¬ì´ì¦ˆë¥¼ ëŠ˜ë¦¬ëŠ” ê²ƒë³´ë‹¤ small single-head self attention ì„ ì¶”ê°€í•˜ëŠ” ê²ƒì´ gMLPê°€ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ê°€ì§€ê²Œ í•œë‹¤ëŠ” ê²ƒì„ ë³´ì„



